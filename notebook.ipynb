{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Básico\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "# Visualização\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pipelines, Transformadores e Modelos\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Métricas de avaliação\n",
    "import shap\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_target_variable_encoder(df, old_target_variable, target_category, new_target_variable=None):\n",
    "    \"\"\"\n",
    "    Function suited for target variables with only two categories (classes). \n",
    "    Encode the given target_variable as 1 and 0. Caution: even missing values \n",
    "    will be encoded as 0.\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Pandas:\n",
    "        import pandas as pd\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        Dataframe that contains all data.\n",
    "    old_target_variable: str\n",
    "        Column name of the targe variable (dependent variable).\n",
    "    target_category: str\n",
    "        Category (class), presented in the target variable, that will be\n",
    "        encoded as 1. The other category will be encoded as 0, even\n",
    "        missing values.\n",
    "    new_target_variable: str\n",
    "        (Optional) New column name of the targe variable (dependent variable).\n",
    "        Default value is None.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with dependent variable (target variable) encoded\n",
    "        as binary column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode target_variable for a binary classification problem\n",
    "    df[old_target_variable] = df[old_target_variable].apply(lambda x: 1 if x == target_category else 0)\n",
    "        \n",
    "    if new_target_variable == None:\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        # Rename target_variable\n",
    "        df.rename(columns={old_target_variable : new_target_variable} , inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def easy_target_variable_organizer(df, target_variable):\n",
    "    \"\"\"\n",
    "    Function that shifts the target_variable column to the last postion of the \n",
    "    dataframe and split independent variables from the target variable (dependent). \n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Pandas:\n",
    "        import pandas as pd\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        Dataframe that contains all data.\n",
    "    target_variable: str\n",
    "        Column name of the target variable (dependent).\n",
    "    target_category:\n",
    "        Category (class), presented in the target variable, that will be\n",
    "        encoded as 1. The other category will be encoded as 0, even\n",
    "        missing values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_x : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables.\n",
    "    df_y : pandas.core.series.Series\n",
    "        Pandas series containing the target variable (dependent).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move target_variable column 'SaidaVoluntaria' to the last position \n",
    "    df = df.reindex(columns = [col for col in df.columns if col != target_variable] + [target_variable])\n",
    "    \n",
    "    # Independent variables\n",
    "    df_x = df.iloc[:, 0:df.shape[1]-1]\n",
    "\n",
    "    # Dependent variable (target_variable)\n",
    "    df_y = df.iloc[:, df.shape[1]-1]\n",
    "\n",
    "    return df_x, df_y\n",
    "\n",
    "def easy_one_hot_encoder(df_x_train, df_x_test, nominal_columns):\n",
    "    \"\"\"\n",
    "    Apply One Hot Encode from scikit-learn for a given list of categorical columns (Nominal). \n",
    "    For more information check the link below:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Pandas:\n",
    "        import pandas as pd\n",
    "    Numpy:\n",
    "        import numpy as np\n",
    "    Scikit-Learn:\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "    Pickle:\n",
    "        import pickle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_x_train : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for training. \n",
    "    df_x_test : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for testing.\n",
    "    nominal_columns : list\n",
    "        List containing the names of all columns to be transformed. Preferably \n",
    "        composed of nominal variables. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_x_train: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns encoded (training set).\n",
    "    df_x_test: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns encoded (testing set).\n",
    "    encoder: sklearn.preprocessing._encoders.OneHotEncoder\n",
    "        Trained encoder. Saved in './encoders_scalers/'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiation\n",
    "    encoder = OneHotEncoder(\n",
    "        categories='auto',  # Categories per feature\n",
    "        drop=None, # Whether to drop one of the features\n",
    "        sparse_output=False, # Will return sparse matrix if set True\n",
    "        dtype='int', # Desired data type of the output\n",
    "        handle_unknown='ignore' # When an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros.\n",
    "    )  \n",
    "\n",
    "    # Fit using train data\n",
    "    encoder.fit(df_x_train[nominal_columns])\n",
    "\n",
    "    # Apply the transformation in train and test data\n",
    "    encoded_array_train = encoder.transform(df_x_train[nominal_columns]) \n",
    "    encoded_array_test = encoder.transform(df_x_test[nominal_columns])\n",
    "\n",
    "    # Build the dataframes preserving original indexes\n",
    "    df_x_train_dummies = pd.DataFrame(encoded_array_train, columns=encoder.get_feature_names_out(), index=df_x_train.index)\n",
    "    df_x_test_dummies = pd.DataFrame(encoded_array_test, columns=encoder.get_feature_names_out(), index=df_x_test.index)\n",
    "    \n",
    "    # Inner join to add the one hot encoded features in the original dataset\n",
    "    df_x_train = df_x_train.join(df_x_train_dummies, how='inner')\n",
    "    df_x_test = df_x_test.join(df_x_test_dummies, how='inner')\n",
    "    \n",
    "    # Removing encoded columns\n",
    "    df_x_train.drop(nominal_columns, axis = 1, inplace = True)\n",
    "    df_x_test.drop(nominal_columns, axis = 1, inplace = True)\n",
    "    \n",
    "    with open('./encoders_scalers/one_hot_encoder.pkl', mode='wb') as file:\n",
    "        pickle.dump(encoder, file)\n",
    "    \n",
    "    return df_x_train, df_x_test\n",
    "\n",
    "\n",
    "def easy_ordinal_encoder(df_x_train, df_x_test, ordinal_columns, ordinal_categories):\n",
    "    \"\"\"\n",
    "    Apply Ordinal Encode from scikit-learn for a given list of columns and categories. \n",
    "    For more information check the link below:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Pandas:\n",
    "        import pandas as pd\n",
    "    Numpy:\n",
    "        import numpy as np\n",
    "    Scikit-Learn:\n",
    "        from sklearn.preprocessing import OrdinalEncoder\n",
    "    Pickle:\n",
    "        import pickle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_x_train : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for training. \n",
    "    df_x_test : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for testing.\n",
    "    ordinal_columns : list\n",
    "        List containing the names of all columns to be transformed. Preferably \n",
    "        composed of ordinal variables. \n",
    "    ordinal_categories : list\n",
    "        List of lists containing all categories in each columns. The order of\n",
    "        each category inside the list dictates her order in the encoding process.\n",
    "        \n",
    "        Ex: weather_list = ['cold', 'warm', hot] will be encoded as\n",
    "            weather_list = [0, 1, 2].\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_x_train: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns encoded (training set).\n",
    "    df_x_test: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns encoded (testing set).\n",
    "    encoder: sklearn.preprocessing._encoders.OrdinalEncoder\n",
    "        Trained encoder. Saved in './encoders_scalers/'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiation\n",
    "    encoder = OrdinalEncoder(\n",
    "        categories= ordinal_categories,\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=np.nan,\n",
    "        encoded_missing_value=np.nan\n",
    "    )\n",
    "\n",
    "    # Fit using train data\n",
    "    encoder.fit(df_x_train[ordinal_columns])\n",
    "\n",
    "    # Apply the transformation in train and test data\n",
    "    encoded_array_train = encoder.transform(df_x_train.loc[:,ordinal_columns]) \n",
    "    encoded_array_test = encoder.transform(df_x_test.loc[:,ordinal_columns])\n",
    "\n",
    "    # Build the dataframes preserving original indexes\n",
    "    df_x_train[ordinal_columns] = pd.DataFrame(encoded_array_train,columns=encoder.get_feature_names_out(), index=df_x_train.index)\n",
    "    df_x_test[ordinal_columns] = pd.DataFrame(encoded_array_test,columns=encoder.get_feature_names_out(), index=df_x_test.index)\n",
    "    \n",
    "    with open('./encoders_scalers/ordinal_encoder.pkl', mode='wb') as file:\n",
    "        pickle.dump(encoder, file)\n",
    "    \n",
    "    return df_x_train, df_x_test\n",
    "\n",
    "\n",
    "def easy_leave_one_out_encoder(df_x_train, df_x_test, df_y_train, method='auto', nominal_columns=[]):\n",
    "    \"\"\"\n",
    "    Apply Leave-One-Out encoder from scikit-learn for a given list of columns. \n",
    "    For more information check the link below:\n",
    "    https://contrib.scikit-learn.org/category_encoders/leaveoneout.html\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Pandas:\n",
    "        import pandas as pd\n",
    "    Category Encoders:\n",
    "        from category_encoders import LeaveOneOutEncoder\n",
    "    Pickle:\n",
    "        import pickle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_x_train : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for training. \n",
    "    df_x_test : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for testing.\n",
    "    df_y_train : pandas.core.series.Series\n",
    "        Pandas series containing the dependent variable for training (target variable).\n",
    "    method : str\n",
    "        Method for selecting columns: 'declarative' or 'auto'.\n",
    "            'declarative': give a list os columns names for encoding.\n",
    "            'auto': all string and categorical columns will be encoded. Default option.\n",
    "    numerical_columns : list\n",
    "        List containing all columns names for encoding. Default option is empty [].\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_x_train: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns encoded (training set).\n",
    "    df_x_test: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns encoded (testing set).\n",
    "    encoder: category_encoders.leave_one_out.LeaveOneOutEncoder\n",
    "        Trained encoder. Saved in './encoders_scalers/'\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'declarative':\n",
    "        # Instantiation\n",
    "        encoder = LeaveOneOutEncoder(cols=nominal_columns, return_df=True)\n",
    "    else:\n",
    "       # Instantiation\n",
    "        encoder = LeaveOneOutEncoder(return_df=True)\n",
    "\n",
    "    # Fit using train data\n",
    "    encoder.fit(df_x_train, df_y_train)\n",
    "\n",
    "    # Apply the transformation in train and test data\n",
    "    df_x_train = encoder.transform(df_x_train)\n",
    "    df_x_test = encoder.transform(df_x_test)\n",
    "    \n",
    "    with open('./encoders_scalers/leave_one_out_encoder.pkl', mode='wb') as file:\n",
    "        pickle.dump(encoder, file)\n",
    "    \n",
    "    return df_x_train, df_x_test\n",
    "\n",
    "\n",
    "def easy_standard_scaler(df_x_train, df_x_test, method='auto', numerical_columns=[]):\n",
    "    \"\"\"\n",
    "    Apply Standard Scaler from scikit-learn for all numerical columns. \n",
    "    For more information check the link below:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Pandas:\n",
    "        import pandas as pd\n",
    "    Category Encoders:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "    Pickle:\n",
    "        import pickle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_x_train : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for training. \n",
    "    df_x_test : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for testing.\n",
    "    method : str\n",
    "        Method for selecting columns: 'declarative' or 'auto'.\n",
    "            'declarative': give a list os columns names for scaling.\n",
    "            'auto': scales all numerical columns. Default option.\n",
    "    numerical_columns : list\n",
    "        List containing all columns names for scaling. Default option is empty [].\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_x_train: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns scaled (training set).\n",
    "    df_x_test: pandas.core.frame.DataFrame\n",
    "        Pandas dataframe with the given columns scaled (testing set).\n",
    "    scaler: sklearn.preprocessing._data.StandardScaler\n",
    "        Trained scaler. Saved in './encoders_scalers/'\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'declarative':\n",
    "        continuous_columns = numerical_columns\n",
    "    else:\n",
    "        continuous_columns = list(df_x_train.select_dtypes(include=['int64', 'float64']).columns)\n",
    "\n",
    "    # Instantiation\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Train and transform each columns separately\n",
    "    for continuous_variable in continuous_columns:\n",
    "        \n",
    "        # Fit using train data\n",
    "        scaler.fit(df_x_train[continuous_variable].array.reshape(-1,1))\n",
    "        \n",
    "        # Apply the scaling in train and test data\n",
    "        df_x_train[continuous_variable] = scaler.transform(df_x_train[continuous_variable].array.reshape(-1,1))\n",
    "        df_x_test[continuous_variable] = scaler.transform(df_x_test[continuous_variable].array.reshape(-1,1))\n",
    "        \n",
    "        with open(f'./encoders_scalers/standard_scaler_{continuous_variable}.pkl', mode='wb') as file:\n",
    "            pickle.dump(scaler, file)\n",
    "        \n",
    "    return df_x_train, df_x_test\n",
    "\n",
    "\n",
    "def opt_moving_thresh_roc(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the optimal probabilistc threshold base on the ROC curve.\n",
    "    For more information check the links below:\n",
    "    https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Numpy:\n",
    "        import numpy as np\n",
    "    Sklearn:\n",
    "        from sklearn.metrics import roc_curve\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    y_test : pandas.core.series.Series\n",
    "        Pandas series containing the dependent variable for test. \n",
    "    y_pred : numpy.ndarray\n",
    "        Array with predictions values inferred on testing data by \n",
    "        a trained model.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    threshold_opt_roc: numpy.float64\n",
    "        Float number that represents the optimal probabilistic \n",
    "        threshold based on the ROC curve.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Construction of the ROC curve data\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "    # Computes the geometric mean\n",
    "    gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "    # Search for the optimal threshold\n",
    "    index = np.argmax(gmean)\n",
    "    threshold_opt_roc = round(thresholds[index], ndigits = 4)\n",
    "    \n",
    "    return threshold_opt_roc\n",
    "\n",
    "\n",
    "def opt_moving_thresh_pr(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the optimal probabilistc threshold base on the Precision-Recall curve.\n",
    "    For more information check the links below:\n",
    "    https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Numpy:\n",
    "        import numpy as np\n",
    "    Sklearn:\n",
    "        from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    y_test : pandas.core.series.Series\n",
    "        Pandas series containing the dependent variable for test. \n",
    "    y_pred : numpy.ndarray\n",
    "        Array with predictions values inferred on testing data by \n",
    "        a trained model.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    threshold_opt_pr: numpy.float64\n",
    "        Float number that represents the optimal probabilistic \n",
    "        threshold based on the Precision-Recall curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construction of the Precision-Recall curve data\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "    # Computes the f-score\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    # Search for the optimal threshold\n",
    "    index = np.argmax(fscore)\n",
    "    threshold_opt_pr = round(thresholds[index], ndigits = 4)\n",
    "    \n",
    "    return threshold_opt_pr\n",
    "\n",
    "\n",
    "def opt_moving_thresh_fs(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the optimal probabilistc threshold base on the F-Score curve.\n",
    "    For more information check the links below:\n",
    "    https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Numpy:\n",
    "        import numpy as np\n",
    "    Sklearn:\n",
    "        from sklearn.metrics import f1_score\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    y_test : pandas.core.series.Series\n",
    "        Pandas series containing the dependent variable for test. \n",
    "    y_pred : numpy.ndarray\n",
    "        Array with predictions values inferred on testing data by \n",
    "        a trained model.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    threshold_opt_fs: numpy.float64\n",
    "        Float number that represents the optimal probabilistic \n",
    "        threshold based on the F-Score curve.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Array for finding the optimal threshold\n",
    "    thresholds = np.arange(0.0, 1.0, 0.0001)\n",
    "    fscore = np.zeros(shape=(len(thresholds)))\n",
    "    \n",
    "    # Fit the model\n",
    "    for index, elem in enumerate(thresholds):\n",
    "        \n",
    "        # Corrected probabilities\n",
    "        y_pred_prob = (y_pred > elem).astype('int')\n",
    "        \n",
    "        # Calculate the f-score\n",
    "        fscore[index] = f1_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Search for the optimal threshold\n",
    "    index = np.argmax(fscore)\n",
    "    threshold_opt_fs = round(thresholds[index], ndigits = 4)\n",
    "\n",
    "    return threshold_opt_fs\n",
    "\n",
    "\n",
    "def shap_summary_plot(model, df_x_train, df_x_test, model_name):\n",
    "    \"\"\"\n",
    "    Save a feature importance plot based on shapley values in './modelo_explicabilidade/'. \n",
    "    For more information check the links below:\n",
    "    https://towardsdatascience.com/introduction-to-shap-values-and-their-application-in-machine-learning-8003718e6827\n",
    "    https://medium.com/swlh/push-the-limits-of-explainability-an-ultimate-guide-to-shap-library-a110af566a02\n",
    "    \n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Pandas:\n",
    "        import pandas as pd\n",
    "    Matplotlib:\n",
    "        import matplotlib.pyplot as plt\n",
    "    SHAP:\n",
    "        import shap\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn model object\n",
    "        Sklearn model. Varies according to the instantiated object.\n",
    "    df_x_train : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for training. \n",
    "    df_x_test : pandas.core.frame.DataFrame\n",
    "        Pandas dataframe containing all independent variables for testing.\n",
    "    model_name : str\n",
    "        Name of the saved model.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit TreeExplainer\n",
    "    explainer = shap.TreeExplainer(model, df_x_train, feature_names=df_x_train.columns.tolist(), model_output='probability')\n",
    "    \n",
    "    # Compute shapley values\n",
    "    shap_values = explainer.shap_values(df_x_test)\n",
    "    \n",
    "    # Build a the feature importance\n",
    "    shap.summary_plot(shap_values, df_x_test, feature_names=df_x_test.columns, plot_type=\"bar\", show=False)\n",
    "    \n",
    "    # Save a pdf for future reference\n",
    "    plt.savefig(f'./model_metrics/summary_plot_{model_name}_weighted.pdf', format='pdf', dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def model_metrics_func(true_class, predicted_class):\n",
    "    \"\"\"\n",
    "    Computes the most relevant metrics for classification tasks.\n",
    "\n",
    "    Necessary Packages\n",
    "    ------------------\n",
    "    Sklearn:\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score,\n",
    "            precision_score,\n",
    "            recall_score,\n",
    "            roc_auc_score,\n",
    "            roc_curve,\n",
    "            precision_recall_curve,\n",
    "            f1_score,\n",
    "            classification_report,\n",
    "            confusion_matrix\n",
    "        )\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_class : pandas.core.series.Series\n",
    "        Object which contains the true classes of the dependent variable.\n",
    "    predicted_class : numpy.ndarray\n",
    "        Object which contains the predicted classes of the dependent variable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_metrics_dict : dict\n",
    "        Dicitionary containing the most relevant classification metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    model_accuracy_score = accuracy_score(true_class, predicted_class)\n",
    "    model_precision_score = precision_score(true_class, predicted_class)\n",
    "    model_recall_score = recall_score(true_class, predicted_class)\n",
    "    model_roc_auc_score = roc_auc_score(true_class, predicted_class)\n",
    "    model_f1_score = f1_score(true_class, predicted_class)\n",
    "    model_error = 1 - model_accuracy_score\n",
    "    model_confidence_interval = 1.96*sqrt((model_error*(1-model_error))/true_class.shape[0]) # (95% confidence)\n",
    "    model_confusion_matrix = confusion_matrix(true_class, predicted_class)\n",
    "    model_classification_report = classification_report(true_class, predicted_class)\n",
    "\n",
    "\n",
    "    model_metrics_dict = {\n",
    "        'model_accuracy_score' : model_accuracy_score,\n",
    "        'model_precision_score' : model_precision_score,\n",
    "        'model_recall_score' : model_recall_score,\n",
    "        'model_roc_auc_score' : model_roc_auc_score,\n",
    "        'model_f1_score' : model_f1_score,\n",
    "        'model_error' : model_error,\n",
    "        'model_confidence_interval' : model_confidence_interval,\n",
    "        'model_confusion_matrix' : model_confusion_matrix,\n",
    "        'model_classification_report' : model_classification_report\n",
    "    }\n",
    "\n",
    "    return model_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier evaluation metric\n",
    "evaluation_metric = 'accuracy'\n",
    "#evaluation_metric_list = ['accuracy']#, 'f1', 'roc_auc', 'precision', 'recall', 'neg_log_loss']\n",
    "\n",
    "#for evaluation_metric in evaluation_metric_list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data saved!\n"
     ]
    }
   ],
   "source": [
    "# Import abt\n",
    "df = pd.read_csv('./database/kaggle_voluntary_turnover_1.csv')\n",
    "\n",
    "# Build target variable\n",
    "df = easy_target_variable_encoder(df=df,\n",
    "                                old_target_variable='left',\n",
    "                                target_category='yes',\n",
    "                                new_target_variable='vonluntary_turnover')\n",
    "\n",
    "# Split between dependent and independent variables\n",
    "X_df, y_df = easy_target_variable_organizer(df=df, target_variable='vonluntary_turnover')\n",
    "\n",
    "# Stratified split of the database between training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.30, stratify = y_df, random_state=42)\n",
    "\n",
    "# Save raw data to GridSearchCV\n",
    "with open('./database/dados_gridsearchcv.pkl', mode= 'wb') as file:\n",
    "    pickle.dump([X_train, y_train, X_test, y_test], file)\n",
    "\n",
    "# Checkpoint\n",
    "print('Raw data saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed data saved!\n"
     ]
    }
   ],
   "source": [
    "# One hot encode for nominal variables\n",
    "columns_nominal = [\n",
    "    'department',\n",
    "    'salary'\n",
    "]\n",
    "\n",
    "X_train, X_test = easy_one_hot_encoder(\n",
    "    df_x_train=X_train,\n",
    "    df_x_test=X_test,\n",
    "    nominal_columns=columns_nominal,\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "columns_numerical = [\n",
    "    'review',\n",
    "    'projects',\n",
    "    'tenure',\n",
    "    'satisfaction',\n",
    "    'avg_hrs_month'\n",
    "]\n",
    "\n",
    "X_train, X_test = easy_standard_scaler(\n",
    "    df_x_train=X_train,\n",
    "    df_x_test=X_test,\n",
    "    method='declarative',\n",
    "    numerical_columns=columns_numerical\n",
    ")\n",
    "\n",
    "# Saves train and test data\n",
    "with open('./database/dados_pre_processados.pkl', mode= 'wb') as file:\n",
    "    pickle.dump([X_train, y_train, X_test, y_test], file)\n",
    "\n",
    "# Checkpoint\n",
    "print('Pre-processed data saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Processing pipeline (gridsearch, training, testing and evaluation) -------------\n",
    "\n",
    "# Input for hyperparameter optimization\n",
    "with open('./database/dados_gridsearchcv.pkl', mode= 'rb') as file:\n",
    "    X_train, y_train, X_test, y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- Preprocessing pipeline integrated with GridSearchCV ---------------------\n",
    "\n",
    "# One hot encode for nominal variables\n",
    "nominal_features = [\n",
    "    'department',\n",
    "    'salary'\n",
    "]\n",
    "\n",
    "onehot_tranformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(\n",
    "                    categories='auto',\n",
    "                    drop=None,\n",
    "                    sparse_output=False,\n",
    "                    dtype='int',\n",
    "                    handle_unknown='ignore'\n",
    "                )\n",
    "        )  \n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_features = (\n",
    "    'review',\n",
    "    'projects',\n",
    "    'tenure',\n",
    "    'satisfaction',\n",
    "    'avg_hrs_month'\n",
    ")\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('sc', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('nominal', onehot_tranformer, nominal_features),\n",
    "        ('scaler', numeric_transformer, numeric_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    'gradientboostingclassifier' : GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'gradientboostingclassifier' : {\n",
    "        #'gbclassifier__boosting_type' : ['gbdt'],# 'dart'],\n",
    "        'gradientboostingclassifier__learning_rate' : [0.3],#, 0.1, 0.01],\n",
    "        #'gbclassifier__objective' : ['binary'],\n",
    "        'gradientboostingclassifier__n_estimators' : [200],#, 300, 500, 700],\n",
    "        'gradientboostingclassifier__max_depth' : [3, 5],#, 7, 9, 11],\n",
    "        #'gbclassifier__num_leaves' : [3, 5],#, 7, 9, 11],\n",
    "        #'gbclassifier__scale_pos_weight' : [balance_scaler],\n",
    "        'gradientboostingclassifier__random_state' : [42]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV done for accuracy!\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_object in estimators.items():\n",
    "\n",
    "    # Pipeline instantiation (preprocessing and processing)\n",
    "    pipe = make_pipeline(\n",
    "        preprocessor,\n",
    "        model_object\n",
    "    )\n",
    "\n",
    "    # Instantiation of data stratification (preserves the proportion \n",
    "    # of classes between training and testing)\n",
    "    stratkf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "    # Instantiation of experiments with GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator = pipe,\n",
    "        param_grid = params[model_name],\n",
    "        scoring = evaluation_metric,\n",
    "        cv = stratkf.split(X_train, y_train),\n",
    "        return_train_score = True,\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'GridSearchCV done for {evaluation_metric}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed data loaded for accuracy!\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-processed base for model training\n",
    "with open('./database/dados_pre_processados.pkl', mode= 'rb') as file:\n",
    "    X_train, y_train, X_test, y_test = pickle.load(file)\n",
    "\n",
    "# Checkpoint\n",
    "print(f'Pre-processed data loaded for {evaluation_metric}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models LGBM and RFE trained for accuracy!\n",
      "Models GBC and RFE saved for accuracy!\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Model training with the best hyperparameters -------------------\n",
    "\n",
    "# Model instantiation\n",
    "gbc= GradientBoostingClassifier(\n",
    "    learning_rate = grid_search.best_params_['gradientboostingclassifier__learning_rate'],\n",
    "    n_estimators = grid_search.best_params_['gradientboostingclassifier__n_estimators'],\n",
    "    max_depth = grid_search.best_params_['gradientboostingclassifier__max_depth'],\n",
    "    random_state = grid_search.best_params_['gradientboostingclassifier__random_state']\n",
    ")\n",
    "\n",
    "# Recursive Feature Elimination (RFE) - Best results with 15 features\n",
    "number_of_features = 15\n",
    "rfe = RFE(estimator=gbc, n_features_to_select=number_of_features, step=1)\n",
    "\n",
    "# Train RFE\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Checkpoint\n",
    "print(f'Models LGBM and RFE trained for {evaluation_metric}!')\n",
    "\n",
    "# Save trained RFE model\n",
    "with open(f'./trained_models/rfe_weighted_{evaluation_metric}.pkl', mode= 'wb') as file:\n",
    "    pickle.dump(rfe, file)\n",
    "\n",
    "# Selection of the most relevant features\n",
    "X_train = X_train.loc[:, rfe.support_]\n",
    "X_test = X_test.loc[:, rfe.support_]\n",
    "\n",
    "# Train the model\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Save trained model\n",
    "with open(f'./trained_models/gradientboostingclassifier_{evaluation_metric}.pkl', mode= 'wb') as file:\n",
    "    pickle.dump(lgbm, file)\n",
    "\n",
    "# Checkpoint\n",
    "print(f'Models GBC and RFE saved for {evaluation_metric}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VoluntaryTurnoverProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
